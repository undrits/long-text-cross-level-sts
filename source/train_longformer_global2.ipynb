{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"train_longformer_global2.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNMDq/1qdV7OdzAmqLCyEkb"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Bu6Dv1GnINvk"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ejsZNCV8kWIu"},"source":["!pip install transformers pytorch-lightning datasets pyarrow jsonlines"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PINaP7eOR61P"},"source":["# install nvidia apex to use mixed precision\n","\n","try:\n","  import apex\n","except:\n","  !git clone https://github.com/NVIDIA/apex\n","  %cd apex\n","  !pip install -v --no-cache-dir ./"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NHOAYvIGScpU"},"source":["# Load Data as HF dataset\n"]},{"cell_type":"code","metadata":{"id":"u1rDeq50mDbk"},"source":["# mount drive to access data\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gsbTQLiPmGgw"},"source":["# load data\n","\n","import datasets\n","\n","DIR = \"/content/gdrive/MyDrive/CUNY_Comp_Ling/advanced_nlp/term_project/data/\"\n","TRAIN = '/content/train.jsonl'\n","DEV = '/content/dev.jsonl'\n","TEST = '/content/test.jsonl'\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D2uEbjEqJknE"},"source":["dataset = datasets.load_dataset('json', data_files={'train': TRAIN, 'validation': DEV, 'test': TEST}) \n","\n","print(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Z_RdBcLSlUa"},"source":["# Tokenize"]},{"cell_type":"code","metadata":{"id":"FFB7kbqMkrNN"},"source":["from transformers import LongformerTokenizer\n","# LongformerTokenizer is identical to RobertaTokenizer (SentencePiece)\n","tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096', sep_token='SEP')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X9S8s0D9k4U4"},"source":["import torch\n","import datasets\n","import random\n","from typing import List, Dict\n","\n","def prep_data(data):\n","    encodings = tokenizer.encode_plus(\n","        data['abstract'], \n","        data['text'],\n","        pad_to_max_length=True, \n","        max_length=4096,\n","        add_special_tokens=True,\n","        return_token_type_ids=False,\n","        return_attention_mask=True,\n","        padding='max_length', \n","        truncation=True,\n","        )\n","    \n","    # set global attention mask to the first token of the text (not abstract)\n","    global_attention_mask = torch.zeros(len(encodings.input_ids), dtype=torch.long)\n","    input_ids = encodings.input_ids\n","    start_of_text = [i for i in range(len(input_ids[2:])) if input_ids[i - 2] == tokenizer.sep_token_id and input_ids[i - 1] == tokenizer.sep_token_id]\n","    start_of_text = start_of_text[0]\n","    # initialize global attention on the 1st of the abstract + 1st of the text\n","    global_attention_mask[1] = 1 \n","    global_attention_mask[start_of_text] = 1\n","    encodings.update({'global_attention_mask': global_attention_mask})\n","\n","    # convert label to float tensor for regression training\n","    targets = torch.tensor(data['label'], dtype=torch.float)\n","    targets.contiguous()\n","    # to match the shape of the input tensor (1,1)\n","    targets = targets.view(-1, 1)\n","    encodings.update({'labels': targets})\n","\n","    return encodings"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ONyV9uRxKysp"},"source":["# convert data\n","\n","train_set = dataset['train']\n","validation_set = dataset['validation']\n","test_set = dataset['test']\n","\n","train_set =  train_set.map(prep_data)\n","validation_set =  validation_set.map(prep_data)\n","test_set = test_set.map(prep_data)\n","\n","columns = ['input_ids', 'attention_mask', 'global_attention_mask', 'labels']\n","train_set.set_format(type='torch', columns=columns)\n","validation_set.set_format(type='torch', columns=columns)\n","test_set.set_format(type='torch', columns=columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfoSQYrxw-JI"},"source":["print(test_set[254])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X2QiCMXoBZSV"},"source":["train_set.shape, validation_set.shape, test_set.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b6NwvXOtSvuy"},"source":["# Create Dataloader"]},{"cell_type":"code","metadata":{"id":"68n5nUlGLFfj"},"source":["from torch.utils.data import RandomSampler, DataLoader\n","\n","batch_size = 1 # batch size of 1 but gradient accumulation to 32\n","\n","train_loader = DataLoader(train_set, batch_size, shuffle=True, num_workers=2)\n","valid_loader = DataLoader(validation_set, batch_size, shuffle=True, num_workers=2)\n","test_loader = DataLoader(test_set, batch_size, shuffle=True, num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZQiXKXaGwEkd"},"source":["# Train\n"]},{"cell_type":"code","metadata":{"id":"EA9V8DD4wGJq"},"source":["import apex\n","import datasets\n","import numpy as np\n","import os\n","import pandas as pd\n","from pathlib import Path\n","import random\n","import tqdm\n","\n","import torch\n","from torch import nn\n","from torch import functional as F\n","from torch.utils.data import (\n","    TensorDataset,\n","    random_split,\n","    RandomSampler,\n","    DataLoader\n",")\n","\n","from transformers import (\n","    LongformerForSequenceClassification,\n","    LongformerModel,\n","    LongformerConfig,\n","    Trainer, \n","    TrainingArguments,\n","    AdamW,\n",")\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MXAQpmgQvhiI"},"source":["# instantiate model\n","\n","lf = LongformerForSequenceClassification.from_pretrained(\n","    'allenai/longformer-base-4096',\n","    gradient_checkpointing=True, # default False, changing to True to use in tandem with mixed precision and gradient accumulation\n","    num_labels=1 # regression\n","    )\n","lf.config"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBVXekxmyzrF"},"source":["# define the training arguments\n","\n","SAVE_PATH = DIR + 'longformer_global2_4096/'\n","\n","training_args = TrainingArguments(\n","    output_dir = SAVE_PATH,\n","    do_train = True,\n","    do_eval = True,\n","    num_train_epochs = 1,\n","    per_device_train_batch_size = 1, # as in the paper\n","    gradient_accumulation_steps = 32, # as in the paper    \n","    per_device_eval_batch_size= 8,\n","    evaluation_strategy = \"steps\",\n","    eval_steps = 100,\n","    disable_tqdm = False, \n","    load_best_model_at_end=True,\n","    learning_rate = 3e-5, # from paper (default = 5e-5)\n","    warmup_steps=len(train_set)//10,\n","    weight_decay=0.01,\n","    logging_steps = 500, # =default\n","    fp16 = True,\n","    fp16_opt_level = 'O1', # default for apex mixed precision\n","    logging_dir= DIR + '/logs/',\n","    dataloader_num_workers = 2,\n","    run_name = 'longformer-global-tuned',\n","\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T2T2A_ph3ojT"},"source":["# resize token embeddings\n","\n","lf.resize_token_embeddings(len(tokenizer))\n","\n","# train\n","trainer = Trainer(\n","    model = lf,\n","    args = training_args,\n","    train_dataset = train_set,\n","    eval_dataset = validation_set,\n",")\n","# set device to cuda\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","trainer.train()\n","\n","# save best model\n","lf.save_pretrained(SAVE_PATH)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-7EF8K3UZF1n"},"source":["import pprint\n","try:\n","  eval = trainer.evaluate()\n","  pprint.pprint(eval)\n","except:\n","  print(\"no eval\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-TeBZH23VV6j"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"5GBCffPvVLGV"},"source":["import datasets\n","import numpy as np\n","\n","metric = datasets.load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gA3-26RrV3wf"},"source":["# save model's predictions\n","\n","import jsonlines\n","import json\n","\n","# predict\n","tester = Trainer(model=lf)\n","predictions = tester.predict(test_set)\n","\n","\n","# save\n","PRED_SAVE_PATH = DIR + \"longformer_global2_4096_predictions.csv\"\n","\n","preds = [pred[0] for pred in predictions.predictions.tolist()]\n","label_ids = [label[0] for label in predictions.label_ids.tolist()]\n","dictionary = {\n","    'gold_labels': label_ids,\n","    'predictions': preds,\n","}\n","df = pd.DataFrame.from_dict(dictionary)\n","df.to_csv(PRED_SAVE_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0wwjyqATaO-"},"source":["predictions.metrics\n"],"execution_count":null,"outputs":[]}]}