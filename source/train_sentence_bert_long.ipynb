{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_sentence_bert_long.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNhsJR7M1CCY9SE703E9OnC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"mSCSBJztqLIH"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BGUa6epoXnfa"},"source":["!pip install -U sentence-transformers jsonlines datasets ipywidgets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GcSacsszRqVr","executionInfo":{"status":"ok","timestamp":1621901009902,"user_tz":240,"elapsed":2853,"user":{"displayName":"Yulia Spektor","photoUrl":"https://lh6.googleusercontent.com/-U5iEGwAoTOc/AAAAAAAAAAI/AAAAAAABWY4/2QdsKenvlRg/s64/photo.jpg","userId":"11042471546002015818"}}},"source":["!pip install tqdm>=4.55.0"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"lyOsdoIVoVP1"},"source":["# install nvidia apex to use mixed precision\n","\n","try:\n","  import apex\n","except:\n","  !git clone https://github.com/NVIDIA/apex\n","  %cd apex\n","  !pip install -v --no-cache-dir ./"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMtWYDFEcKAw"},"source":["# mount drive to access data\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GI9OAtFed4j7"},"source":["# Load data"]},{"cell_type":"code","metadata":{"id":"Q6iaQ8oOcMWM","executionInfo":{"status":"ok","timestamp":1621901066310,"user_tz":240,"elapsed":5046,"user":{"displayName":"Yulia Spektor","photoUrl":"https://lh6.googleusercontent.com/-U5iEGwAoTOc/AAAAAAAAAAI/AAAAAAABWY4/2QdsKenvlRg/s64/photo.jpg","userId":"11042471546002015818"}}},"source":["# load data\n","import jsonlines\n","from sentence_transformers.readers import InputExample\n","from typing import List, Tuple\n","\n","DIR = \"/content/gdrive/MyDrive/CUNY_Comp_Ling/advanced_nlp/term_project/data/\"\n","TRAIN = 'train.jsonl'\n","DEV = 'dev.jsonl'\n","TEST = 'test.jsonl'\n","\n","def construct_examples(filepath: str) -> List[InputExample]:\n","  examples = []\n","  with jsonlines.open(filepath) as source:\n","    for line in source.iter():\n","      abstract = line['abstract']\n","      text = line['text']\n","      label = line['label']\n","      examples.append(InputExample(texts=[abstract, text], label=label))\n","  return examples\n","\n","def construct_eval_examples(filepath: str) -> Tuple[List, List, List]:\n","  abstracts = []\n","  texts = []\n","  labels = []\n","  with jsonlines.open(filepath) as source:\n","    for line in source.iter():\n","      abstracts.append(line['abstract'])\n","      texts.append(line['text'])\n","      labels.append(line['label'])\n","  return abstracts, texts, labels"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8N4ABVunIje","executionInfo":{"status":"ok","timestamp":1621901221181,"user_tz":240,"elapsed":1720,"user":{"displayName":"Yulia Spektor","photoUrl":"https://lh6.googleusercontent.com/-U5iEGwAoTOc/AAAAAAAAAAI/AAAAAAABWY4/2QdsKenvlRg/s64/photo.jpg","userId":"11042471546002015818"}}},"source":["train_examples = construct_examples(DIR + TRAIN)\n","dev_examples = construct_eval_examples(DIR + DEV)\n","test_examples = construct_eval_examples(DIR + TEST)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"M3ZkoBwoqhjQ"},"source":["len(train_examples), len(test_examples[0]), len(dev_examples[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RzYyshwdk9_T"},"source":["# Train"]},{"cell_type":"code","metadata":{"id":"xFTUtt_EhA1f"},"source":["import datasets\n","import numpy as np\n","import os\n","import pandas as pd\n","from pathlib import Path\n","import random\n","import tqdm\n","\n","import torch\n","from torch import nn\n","from torch import functional as F\n","from torch.utils.data import (\n","    TensorDataset,\n","    random_split,\n","    RandomSampler,\n","    DataLoader\n",")\n","\n","from transformers import (\n","    LongformerForSequenceClassification,\n","    LongformerModel,\n","    LongformerConfig,\n","    Trainer, \n","    TrainingArguments,\n","    AdamW,\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cw6oHPJWk_fc","executionInfo":{"status":"ok","timestamp":1621901094150,"user_tz":240,"elapsed":229,"user":{"displayName":"Yulia Spektor","photoUrl":"https://lh6.googleusercontent.com/-U5iEGwAoTOc/AAAAAAAAAAI/AAAAAAABWY4/2QdsKenvlRg/s64/photo.jpg","userId":"11042471546002015818"}}},"source":["from sentence_transformers import SentenceTransformer, losses, evaluation, models\n","from torch.utils.data import DataLoader\n","from typing import Dict, Union\n","\n","\n","def regression_training(\n","    model_name_or_path: str, \n","    batch_size: int,\n","    hparams: Dict[str, Union[str, int, bool]],\n","    embedding_model_args: Dict[str, Union[str, int, bool]],\n","    train_examples, dev_examples\n","    ):\n","    # initiate with Longformer\n","    embedding_model = models.Transformer(\n","        model_name_or_path,\n","        model_args=embedding_model_args) \n","    pooling_model = models.Pooling(embedding_model.get_word_embedding_dimension()) \n","    dense_model = models.Dense(\n","        in_features=pooling_model.get_sentence_embedding_dimension(), \n","        out_features=256, \n","        activation_function=nn.Tanh()\n","        )\n","    modules = [\n","               embedding_model,\n","               pooling_model,\n","               dense_model\n","               ]\n","    regression_model = SentenceTransformer(modules=modules)\n","\n","    #define dataloader and loss\n","    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n","    train_loss = losses.CosineSimilarityLoss(regression_model)\n","\n","    # evaluator\n","    evaluator = evaluation.EmbeddingSimilarityEvaluator(\n","        dev_examples[0], \n","        dev_examples[1], \n","        dev_examples[2]\n","    )\n","\n","    # train\n","    regression_model.fit(\n","        train_objectives=[(train_dataloader, train_loss)], \n","        epochs=hparams['epochs'], \n","        warmup_steps=hparams['warmup_steps'],\n","        scheduler=hparams['scheduler'],\n","        evaluator=evaluator,\n","        evaluation_steps=hparams['eval_steps'],\n","        output_path=hparams['output_path'],\n","        save_best_model=hparams['save_best_model'],\n","        use_amp=hparams['use_amp'] if 'use_amp' in hparams else False,\n","        checkpoint_path=hparams['checkpoint_path']\n","        )\n","\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"sa9VmxDeyBlj"},"source":["MODEL_SAVE_PATH = DIR + \"sbert_long/\"\n","\n","# hparams from the paper\n","hyperparams = {\n","    'epochs': 1,\n","    'scheduler': 'WarmupLinear',\n","    'warmup_steps': len(train_examples) // 10,\n","    # optimizer = Adam by default\n","    # optimizer_params = {'lr': 2e-05} by default\n","    'eval_steps': 100,\n","    'output_path': MODEL_SAVE_PATH,\n","    'save_best_model': True,\n","    'use_amp': True, # mixed precision\n","    'checkpoint_path': MODEL_SAVE_PATH,\n","}\n","\n","# longformer args\n","embedding_model_args = {\n","    'gradient_checkpointing':True,\n","    \"num_labels\": 1,\n","    \"max_length\": 1024,\n","}\n","\n","# train\n","regression_training(\n","    model_name_or_path='allenai/longformer-base-4096', \n","    batch_size=4,\n","    hparams=hyperparams,\n","    embedding_model_args=embedding_model_args,\n","    train_examples=train_examples,\n","    dev_examples=dev_examples\n","    )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kYSM6RhV2pyj","executionInfo":{"status":"ok","timestamp":1621909730356,"user_tz":240,"elapsed":446,"user":{"displayName":"Yulia Spektor","photoUrl":"https://lh6.googleusercontent.com/-U5iEGwAoTOc/AAAAAAAAAAI/AAAAAAABWY4/2QdsKenvlRg/s64/photo.jpg","userId":"11042471546002015818"}}},"source":["# save test predictions\n","\n","import numpy as np\n","import pandas as pd\n","import time\n","from sentence_transformers import util\n","\n","def sts_similarity(sent_1: str, sent_2: str, model: SentenceTransformer):\n","  emb1 = model.encode(sent_1)\n","  emb2 = model.encode(sent_2)\n","  cos_sim = util.pytorch_cos_sim(emb1, emb2)\n","  return np.array(cos_sim)[0][0]\n","\n","def save_predictions(gold_data_path: str, pred_save_path: str,\n","                     model: SentenceTransformer):\n","  df = pd.DataFrame(columns=['gold_labels', 'predictions'])\n","  count = 0\n","  start_time = time.process_time()\n","  with jsonlines.open(gold_data_path) as f:\n","    for line in f.iter():\n","        abstract = line['abstract']\n","        text = line['text']\n","        label = line['label']\n","        sts = sts_similarity(abstract, text, model)\n","        results = {\n","            'gold_labels': label,\n","            'predictions': sts,\n","        }\n","        df = df.append(results, ignore_index=True)\n","        if not count % 100:\n","          print(f\"processed {count} texts in {time.process_time() - start_time}\")\n","          start_time = time.process_time()\n","        count += 1\n","    df.to_csv(pred_save_path)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"2CLUpPhW3LQS"},"source":["PRED_SAVE_PATH = 'sbert_long_predictions.csv'\n","\n","best_regression_model = SentenceTransformer(MODEL_SAVE_PATH)\n","\n","save_predictions(DIR + TEST, DIR + PRED_SAVE_PATH, best_regression_model)"],"execution_count":null,"outputs":[]}]}