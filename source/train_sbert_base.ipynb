{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_sbert_base.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyP72YBUoyE4uEAuFzXdI8uN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"BGUa6epoXnfa"},"source":["pip install -U sentence-transformers jsonlines"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMtWYDFEcKAw"},"source":["# mount drive to access data\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GI9OAtFed4j7"},"source":["# Load data"]},{"cell_type":"code","metadata":{"id":"Q6iaQ8oOcMWM"},"source":["# load data\n","import jsonlines\n","from sentence_transformers.readers import InputExample\n","from typing import List, Tuple\n","\n","DIR = \"/content/gdrive/MyDrive/CUNY_Comp_Ling/advanced_nlp/term_project/data/\"\n","TRAIN = 'train.jsonl'\n","DEV = 'dev.jsonl'\n","TEST = 'test.jsonl'\n","\n","def construct_examples(filepath: str) -> List[InputExample]:\n","  examples = []\n","  with jsonlines.open(filepath) as source:\n","    for line in source.iter():\n","      abstract = line['abstract']\n","      text = line['text']\n","      label = line['label']\n","      examples.append(InputExample(texts=[abstract, text], label=label))\n","  return examples\n","\n","def construct_eval_examples(filepath: str) -> Tuple[List, List, List]:\n","  abstracts = []\n","  texts = []\n","  labels = []\n","  with jsonlines.open(filepath) as source:\n","    for line in source.iter():\n","      abstracts.append(line['abstract'])\n","      texts.append(line['text'])\n","      labels.append(line['label'])\n","  return abstracts, texts, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f8N4ABVunIje"},"source":["train_examples = construct_examples(DIR + TRAIN)\n","dev_examples = construct_eval_examples(DIR + DEV)\n","test_examples = construct_eval_examples(DIR + TEST)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M3ZkoBwoqhjQ"},"source":["len(train_examples), len(dev_examples[0]), len(test_examples[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RzYyshwdk9_T"},"source":["# Train"]},{"cell_type":"code","metadata":{"id":"cw6oHPJWk_fc"},"source":["from sentence_transformers import SentenceTransformer, losses, evaluation\n","from torch.utils.data import DataLoader\n","from typing import Dict, Union\n","\n","\n","def regression_training(\n","    model_name_or_path: str, \n","    batch_size: int,\n","    hparams: Dict[List, Union[str, int, bool]]\n","    ):\n","    \n","    regression_model = SentenceTransformer(model_name_or_path)\n","\n","    #define dataloader and loss\n","    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n","    train_loss = losses.CosineSimilarityLoss(regression_model)\n","\n","    # evaluator\n","    evaluator = evaluation.EmbeddingSimilarityEvaluator(\n","        dev_examples[0], \n","        dev_examples[1], \n","        dev_examples[2]\n","    )\n","\n","    # train\n","    regression_model.fit(\n","        train_objectives=[(train_dataloader, train_loss)], \n","        epochs=hparams['epochs'], \n","        warmup_steps=hparams['warmup_steps'],\n","        scheduler=hparams['scheduler'],\n","        evaluator=evaluator,\n","        evaluation_steps=hparams['eval_steps'],\n","        output_path=hparams['output_path'],\n","        save_best_model=hparams['save_best_model'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z41Yx0yzCsQc"},"source":["# train\n","\n","# hparams from the paper\n","HYPERPARAMS = {\n","    'epochs': 1,\n","    'scheduler': 'WarmupLinear',\n","    'warmup_steps': len(train_examples) // 10,\n","    # optimizer = Adam by default\n","    # optimizer_params = {'lr': 2e-05} by default\n","    'eval_steps': 500,\n","    'output_path': DIR + \"sbert_base/\",\n","    'save_best_model': True\n","}\n","\n","regression_training(\n","    model_name_or_path='roberta-base', # Training the RoBERTa-based model\n","    batch_size=8,\n","    hparams=HYPERPARAMS,\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dh5tvUjmDDOf"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"kYSM6RhV2pyj"},"source":["# save test predictions\n","\n","import numpy as np\n","import pandas as pd\n","import time\n","from sentence_transformers import util\n","\n","def sts_similarity(sent_1: str, sent_2: str, model: SentenceTransformer):\n","  emb1 = model.encode(sent_1)\n","  emb2 = model.encode(sent_2)\n","  cos_sim = util.pytorch_cos_sim(emb1, emb2)\n","  return np.array(cos_sim)[0][0]\n","\n","def save_predictions(gold_data_path: str, pred_save_path: str,\n","                     model: SentenceTransformer):\n","  df = pd.DataFrame(columns=['gold_labels', 'predictions'])\n","  count = 0\n","  start_time = time.process_time()\n","  with jsonlines.open(gold_data_path) as f:\n","    for line in f.iter():\n","        abstract = line['abstract']\n","        text = line['text']\n","        label = line['label']\n","        sts = sts_similarity(abstract, text, model)\n","        results = {\n","            'gold_labels': label,\n","            'predictions': sts,\n","        }\n","        df = df.append(results, ignore_index=True)\n","        if not count % 100:\n","          print(f\"processed {count} texts in {time.process_time() - start_time}\")\n","          start_time = time.process_time()\n","        count += 1\n","    df.to_csv(pred_save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2CLUpPhW3LQS"},"source":["PRED_SAVE_PATH = 'sbert_base_predictions.csv'\n","\n","save_predictions(DIR + TEST, DIR + PRED_SAVE_PATH, best_regression_model)"],"execution_count":null,"outputs":[]}]}